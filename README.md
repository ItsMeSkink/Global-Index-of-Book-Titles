The attempt to use random proxies has descended into failure. Attempting many combinations of proxies and recurring functions has yielded no or unsatisfactory results which doesn't at all promise a fundamental base for the GIBT algorithm. Much of the problem lies in the uncertainity of finding a valid proxy, inability to scrape Amazon through a proxy or repeatedly and a huge delay in the intitiation of the code and the intended results. 

The mistake has thus far been patience for the promised chunk of code to work even after conservative changes and not keeping the development parallel to the code runner. 

The next step would be to focus on pre-existing scrappers such as "smartproxy" which has yielded the required results. We will go on to assess the data that SmartProxy provides and provide alternatively a much structured data focussing on Books.

After structuring the data, further time would be invested in Research and Development of string operations and scaling of the GIBT algorithm